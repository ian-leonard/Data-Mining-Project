---
title: "542 project"
author: "Jesse Long"
date: "2023-11-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
master <- read.csv("Master.csv")
batting <- read.csv("Batting.csv")
salaries <- read.csv("Salaries.csv")
teams <- read.csv("Teams.csv")
pitching <- read.csv("Pitching.csv")
allstar <- read.csv("AllstarFull.csv")



library(tidyverse)
library(car)
library(ggplot2)
library(caret)

```


```{r}
library(dplyr)
```


Predicting Team Batting Average with linear regression


Feature Engineering/Filtering
```{r}

teams<-teams %>% filter(yearID>=1980)
nrow(teams)
teams$AVG<- teams$H/teams$AB

head(teams)


names(teams)
#Taking out pitching info/unimportant columns
teamspredicting<-teams[-c(3,4,6,7,8,10,11,12,13,14,16,17,25,26,28,33,34,35,36,37,38,41,42,44,46,47,48)]


#Checking for missing values
length(which(is.na(teamspredicting==TRUE)))
teamspredicting[which(is.na(teamspredicting==TRUE)),]



teamspredicting_clean <- na.omit(teamspredicting)


head(teamspredicting_clean)
```


Model Building
```{r}
nTotal<- nrow(teamspredicting_clean)


nTrain<-712#training data


nTest<-306 #test data


set.seed(541)
train<-sample(nTotal, nTrain)



model<-lm(AVG~., data<-teamspredicting_clean[train,]) #builds linear model just off training data



summary(model)



#Examining for multicolinearity
vif(model)



## testing
pred<-predict(model, newdata<-teamspredicting_clean[-train,])# makes prediction (applies model) on just testing data


diff<-teamspredicting_clean$AVG[-train]-pred



error<-sqrt(sum(diff**2)/nTest)

cat(sprintf('nTrain=%d, nTest=%d, error=%f\n', nTrain, nTest, error))


order(diff,decreasing=TRUE)


max(diff)

newdata$diff<- diff

newdata$predictedAVG<- pred



extracted_rows <- teams[rownames(newdata), ]

```

Cross Validation 

```{r}
fit <- train(AVG ~ ., data=teamspredicting_clean, method='lm', trControl = trainControl(method='cv', number=10))


```

Plotting predicted AVG vs observed AVG for teams in Test Data

```{r}

newdata$Type <- rep("Observed", nrow(newdata))
newdata$Type[which(!is.na(newdata$predictedAVG))] <- "Predicted"

ggplot(newdata, aes(x=yearID)) +
  geom_smooth(aes(y=AVG, color="Observed"), se=FALSE) +
  geom_smooth(aes(y=predictedAVG, color="Predicted"), se=FALSE) +
  labs(x="Year", y="Batting AVG", title="Predicted AVG vs Observed AVG") +
  scale_color_manual(values=c("Observed"="red", "Predicted"="black")) +
  theme(legend.title=element_blank()) # Optional: remove the legend title if desired



```

Biggest Misses on New Data

```{r}

extracted_rows <- teams[rownames(newdata), ]
order(abs(diff),decreasing=TRUE)


#1994 Padres
extracted_rows[114,]
newdata[114,]

#1990 Mets
extracted_rows[85,]
newdata[85,]

#1985 White Sox
extracted_rows[34,]
newdata[34,]

#1982 Oakland A's
extracted_rows[14,]
newdata[14,]

#2002 Indians
extracted_rows[180,]
newdata[180,]
```
1. 1994 Padres: Observed AVG(0.2745821) Predicted AVG(0.2570354) Difference:0.0175467
2. 1990 Mets:   Observed AVG(0.2533821) Predicted AVG(0.2698214) Difference:-0.01364408
3. 1985 White Sox: Observed AVG(0.2533821) Predicted AVG(0.2665708) Difference:-0.01318867  
4. 1982 A's:    Observed AVG(0.2360499) Predicted AVG(0.2491338) Difference:-0.01308389
5. 2002 Indians:Observed AVG(0.2487553) Predicted AVG(0.2617859) Difference: -0.01303057


```{r}
summary(model)
```

Statistically Significant Predictors:
Runs: More Runs -> Higher Batting Average
HR:   Less Home Runs -> Higher Batting Average
BB:   Less Walks -> Higher Batting Average
SO: More Strike Outs -> Lower Batting
SB: More Stolen Bases -> Lower Batting Average
RA: More runs given up -> Lower Batting Average
ERA: Higher ERA -> Higher Batting Average
attendance: More fans -> Higher Batting Average




Lab example
```{r}
model<-lm(Price~., data=auto)
summary(model)
#nothing predictive here. Just a descriptive model
```

```{r}
nTrain<-1000 #training data
nTest<-nTotal-nTrain #test data
nTotal<-nrow(auto)

nTrain<-1000 #training data
nTest<-nTotal-nTrain #test data

# random selection for training and testing data
# optionally, fixing the seed value guarantees the same results in repeated runs
set.seed(1)
train<-sample(nTotal, nTrain) #randomly selects data. 1st argument is the data u want to select from. Second argument is how many values you want

## training
model<-lm(Price~., data<-auto[train,]) #builds linear model just off training data
summary(model)

## testing
pred<-predict(model, newdata<-auto[-train,])# makes prediciton (applies model) on just testing data


# evaluation with root mean square error
diff<-auto$Price[-train]-pred
error<-sqrt(sum(diff**2)/nTest)

cat(sprintf('nTrain=%d, nTest=%d, error=%f\n', nTrain, nTest, error))

vif(model)
```




