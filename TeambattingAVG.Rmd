---
title: "542 project"
author: "Jesse Long"
date: "2023-11-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
master <- read.csv("Master.csv")
batting <- read.csv("Batting.csv")
salaries <- read.csv("Salaries.csv")
teams <- read.csv("Teams.csv")
pitching <- read.csv("Pitching.csv")
allstar <- read.csv("AllstarFull.csv")



library(tidyverse)
library(car)
library(ggplot2)
library(caret)
library(randomForest)

```


```{r}
library(dplyr)
```


Predicting Team Batting Average with linear regression


Feature Engineering/Filtering
```{r}

teams<-teams %>% filter(yearID>=1980)
nrow(teams)
teams$AVG<- teams$H/teams$AB

head(teams)


names(teams)
#Taking out pitching info/unimportant columns
teamspredicting<-teams[-c(3,4,6,7,8,10,11,12,13,14,16,17,25,26,28,33,34,35,36,37,38,41,42,45,46,47,48)]


#Checking for missing values
length(which(is.na(teamspredicting==TRUE)))
teamspredicting[which(is.na(teamspredicting==TRUE)),]




teamspredicting_clean <- na.omit(teamspredicting)


head(teamspredicting_clean)
```


Cross Validation Linear Regression

```{r}
#fit <- train(AVG ~ ., data=teamspredicting_clean, method='lm', trControl = trainControl(method='cv', number=5))

#summary(fit)


set.seed(541)  # Set a seed for reproducibility

# Determine the number of rows for the training set

nTotal <- nrow(teamspredicting_clean)
nTrain <- 712
nTest<-nTotal-nTrain
# Generate random indices for the training set
train <- sample(nTotal, nTrain)

# Create the training set and test set based on the indices
trainData <- teamspredicting_clean[train, ]
testData <- teamspredicting_clean[-train, ]


ctrl <- trainControl(
  method = "cv",    # Cross-validation method ("cv" for k-fold)
  number = 10,       # Number of folds
  verboseIter = TRUE # Show progress during training
)


model <- train(
  AVG ~ .,             # Formula for your model
  data = trainData,  # Training data
  method = "lm",    # Random Forest method
  trControl = ctrl  # Cross-validation control
)


print(model)


pred <- predict(model, newdata = testData)

diff<-teamspredicting_clean$AVG[-train]-pred



error<-sqrt(sum(diff**2)/nTest)

cat(sprintf('nTrain=%d, nTest=%d, error=%f\n', nTrain, nTest, error))


order(abs(diff),decreasing=TRUE)


newdata$diff<- diff

newdata$predictedAVG<- pred



extracted_rows <- teams[rownames(newdata), ]

```

Plotting predicted AVG vs observed AVG for teams in Test Data

```{r}

newdata$Type <- rep("Observed", nrow(newdata))
newdata$Type[which(!is.na(newdata$predictedAVG))] <- "Predicted"

ggplot(newdata, aes(x=yearID)) +
  geom_smooth(aes(y=AVG, color="Observed"), se=FALSE) +
  geom_smooth(aes(y=predictedAVG, color="Predicted"), se=FALSE) +
  labs(x="Year", y="Batting AVG", title="Predicted AVG vs Observed AVG") +
  scale_color_manual(values=c("Observed"="red", "Predicted"="black")) +
  theme(legend.title=element_blank()) # Optional: remove the legend title if desired



```

Biggest Misses on New Data

```{r}

extracted_rows <- teams[rownames(newdata), ]
order(abs(diff),decreasing=TRUE)


#1994 Padres
extracted_rows[114,]
newdata[114,]

#1990 Mets
extracted_rows[85,]
newdata[85,]

#1985 White Sox
extracted_rows[34,]
newdata[34,]

#1982 Oakland A's
extracted_rows[14,]
newdata[14,]

#2002 Indians
extracted_rows[180,]
newdata[180,]
```
1. 1994 Padres: Observed AVG(0.2745821) Predicted AVG(0.2570354) Difference:0.0175467
2. 1990 Mets:   Observed AVG(0.2561773) Predicted AVG(0.2698214) Difference:-0.01364408
3. 1985 White Sox: Observed AVG(0.2533821) Predicted AVG(0.2665708) Difference:-0.01318867  
4. 1982 A's:    Observed AVG(0.2360499) Predicted AVG(0.2491338) Difference:-0.01308389
5. 2002 Indians:Observed AVG(0.2487553) Predicted AVG(0.2617859) Difference: -0.01303057


```{r}
summary(model)
```



Statistically Significant Predictors:
Runs: More Runs -> Higher Batting Average
HR:   Less Home Runs -> Higher Batting Average
BB:   Less Walks -> Higher Batting Average
SO: More Strike Outs -> Lower Batting
SB: More Stolen Bases -> Lower Batting Average
RA: More runs given up -> Lower Batting Average
ERA: Higher ERA -> Higher Batting Average
attendance: More fans -> Higher Batting Average



Random Forest
```{r}

set.seed(541)  # Set a seed for reproducibility

# Determine the number of rows for the training set

nTotal <- nrow(teamspredicting_clean)
nTrain <- 712
nTest<-nTotal-nTrain
# Generate random indices for the training set
train <- sample(nTotal, nTrain)

# Create the training set and test set based on the indices
trainData <- teamspredicting_clean[train, ]
testData <- teamspredicting_clean[-train, ]


ctrl <- trainControl(
  method = "cv",    # Cross-validation method ("cv" for k-fold)
  number = 5,       # Number of folds
  verboseIter = TRUE # Show progress during training
)


model <- train(
  AVG ~ .,             # Formula for your model
  data = trainData,  # Training data
  method = "rf",    # Random Forest method
  trControl = ctrl  # Cross-validation control
)


print(model)


pred <- predict(model, newdata = testData)


## testing
pred<-predict(model, newdata<-teamspredicting_clean[-train,])# makes prediction (applies model) on just testing data


diff<-teamspredicting_clean$AVG[-train]-pred

sd(diff)

error<-sqrt(sum(diff**2)/nTest)

cat(sprintf('nTrain=%d, nTest=%d, error=%f\n', nTrain, nTest, error))


varImp(model)
```


One Standard Deviation Rule

```{r}


rmse_linear <- 0.004752
rmse_rf <- 0.006043

rmse_sd <- sd(c(rmse_linear, rmse_rf))
upper_bound <- rmse_linear + rmse_sd
lower_bound <- rmse_linear - rmse_sd


if (rmse_rf <= upper_bound && rmse_rf >= lower_bound) {
  cat("Random Forest Model is within one standard deviation of the Linear Model.\n")
} else if (rmse_linear <= upper_bound && rmse_linear >= lower_bound) {
  cat("Linear Model is within one standard deviation of the Random Forest Model.\n")
} else {
  cat("Neither model is within one standard deviation of the other.\n")
}
```




Lab example
```{r}
model<-lm(Price~., data=auto)
summary(model)
#nothing predictive here. Just a descriptive model
```

```{r}
nTrain<-1000 #training data
nTest<-nTotal-nTrain #test data
nTotal<-nrow(auto)

nTrain<-1000 #training data
nTest<-nTotal-nTrain #test data

# random selection for training and testing data
# optionally, fixing the seed value guarantees the same results in repeated runs
set.seed(1)
train<-sample(nTotal, nTrain) #randomly selects data. 1st argument is the data u want to select from. Second argument is how many values you want

## training
model<-lm(Price~., data<-auto[train,]) #builds linear model just off training data
summary(model)

## testing
pred<-predict(model, newdata<-auto[-train,])# makes prediciton (applies model) on just testing data


# evaluation with root mean square error
diff<-auto$Price[-train]-pred
error<-sqrt(sum(diff**2)/nTest)

cat(sprintf('nTrain=%d, nTest=%d, error=%f\n', nTrain, nTest, error))


```




